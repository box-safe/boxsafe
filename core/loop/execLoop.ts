/**
 * @fileoverview
 * Implements an iterative agent loop that coordinates LLM code generation,
 * execution, and validation until a successful result is achieved.
 *
 * @description
 * The loop follows a deterministic pipeline:
 * 1. Send feedback to an LLM instance.
 * 2. Read the generated markdown artifact.
 * 3. Extract language-specific code blocks.
 * 4. Write the extracted code to an output file.
 * 5. Execute the output via a system command.
 * 6. Validate execution results using a waterfall validator.
 * 7. Generate structured feedback on failure and retry.
 *
 * The process repeats until validation succeeds.
 *
 * This module is designed for automated agent workflows.
 * Logs and artifacts are intended for machine consumption, not humans.
 *
 * @module core/loop
 */

import { readFile, writeFile, rename } from "node:fs/promises";
import { pathToCode } from "@core/paths/paths";
import { createLLM } from "@ai/provider";
import { runLLM } from "@ai/caller";
import { LService, LModel } from "@ai/label";
import { extractCode } from "@/util/extractCode";
import type { CommandRun } from "@/types";
import { execode } from "@core/cmd/execode";
import { ANSI } from "@util/ANSI";
import { waterfall } from "@core/loop/waterfall";

interface LoopOptions {
  service: LService;
  model: LModel;
  initialPrompt: string;
  cmd: CommandRun;
  lang: string;
  pathOutput: string;
  // optional safety / configuration
  maxIterations?: number;
  // number of iterations to run the main loop (overrides maxIterations when provided)
  limit?: number;
  signal?: AbortSignal;
  // allow a custom path for the model-generated markdown (defaults to `pathToCode`)
  pathGeneratedMarkdown?: string;
  logger?: {
    info: (...args: any[]) => void;
    warn: (...args: any[]) => void;
    error: (...args: any[]) => void;
    debug?: (...args: any[]) => void;
  };
}

export interface LoopResult {
  ok: boolean;
  iterations: number;
  verdict?: any;
  artifacts?: { outputFile?: string };
}

// Reads the markdown file generated by the model (path injectable for testability)
const readMarkdown = async (markdownPath: string) => {
  return readFile(markdownPath, "utf-8");
};

export const loop = async (
  {
    service,
    model,
    initialPrompt,
    cmd,
    lang,
    pathOutput,
    maxIterations = 10,
    limit,
    signal,
    pathGeneratedMarkdown = pathToCode,
    logger,
  } : LoopOptions
): Promise<LoopResult> => {
  const llm = createLLM(service, model);
  const log = logger ?? console;

  const effectiveLimit = typeof limit === "number" ? limit : maxIterations;
  let feedback = initialPrompt;

  const sleep = (ms: number) => new Promise((r) => setTimeout(r, ms));

  for (limit = 1; limit <= effectiveLimit; limit++) {
    if (signal?.aborted) {
      log.error(`${ANSI.Red}[Agent]${ANSI.Reset} aborted`);
      return { ok: false, iterations: limit };
    }

    log.info(`${ANSI.Cyan}[Agent]${ANSI.Reset} iteration ${limit}`);

    // Run the LLM with the current feedback (with a small retry/backoff for transient LLM errors)
    let llmAttempts = 0;
    const maxLlmAttempts = 3;
    while (true) {
      try {
        if (signal?.aborted) throw new Error("Aborted");
        // Ensure the model (or mock) receives an explicit instruction about
        // the requested code language so we consistently get the correct
        // fenced block back. This helps the mock and real LLMs produce the
        // expected language output regardless of previous feedback text.
        const promptToSend = `${feedback}\n\nRespond ONLY with a single code block in the language: ${lang}`;
        await runLLM(promptToSend, llm);
        break;
      } catch (err: any) {
        llmAttempts++;
        log.error(`${ANSI.Red}[LLM Runner]${ANSI.Reset}`, err?.message ?? err);
        if (llmAttempts >= maxLlmAttempts) {
          return { ok: false, iterations: limit };
        }
        const backoff = 200 * Math.pow(2, llmAttempts - 1);
        await sleep(backoff);
      }
    }

    // Read the generated markdown
    let markdown: string;
    try {
      if (signal?.aborted) throw new Error("Aborted");
      markdown = await readMarkdown(pathGeneratedMarkdown);
    } catch (err: any) {
      log.error(`${ANSI.Red}[ReadMarkdown]${ANSI.Reset}`, err?.message ?? err);
      // give feedback to the model and retry
      feedback = "Could not read the generated markdown. Please emit markdown artifact.";
      continue;
    }

    // Extract code blocks for the target language
    let codeBlocks: string[];
    try {
      if (signal?.aborted) throw new Error("Aborted");
      codeBlocks = await extractCode(markdown, lang, {
        throwOnNotFound: true,
      });
    } catch (err: any) {
      log.warn(`${ANSI.Yellow}[ExtractCode]${ANSI.Reset}`, err?.message ?? err);
      feedback = "No code blocks were found. Generate valid code for the requested language.";
      continue;
    }

    if (!codeBlocks || codeBlocks.length === 0) {
      feedback = "No code blocks were found. Generate valid code.";
      continue;
    }

    // Atomic write: write to temp and rename
    const tmpPath = `${pathOutput}.tmp`;
    try {
      if (signal?.aborted) throw new Error("Aborted");
      await writeFile(tmpPath, codeBlocks.join("\n\n"), "utf-8");
      await rename(tmpPath, pathOutput);
    } catch (err: any) {
      log.error(`${ANSI.Red}[WriteFile]${ANSI.Reset}`, err?.message ?? err);
      feedback = "Failed to write output file. Ensure filesystem permissions are correct.";
      continue;
    }

    // Execute the generated code
    let execResult: any;
    try {
      if (signal?.aborted) throw new Error("Aborted");
      // If caller left the default `echo OK`, automatically run the
      // generated output file according to the requested language so the
      // loop actually executes the produced artifact.
      let execCmd = cmd;
      if (typeof cmd === 'string' && cmd === 'echo OK') {
        if (lang === 'ts') execCmd = `ts-node ${pathOutput}`;
        else if (lang === 'js') execCmd = `node ${pathOutput}`;
        else if (lang === 'py' || lang === 'python') execCmd = `python ${pathOutput}`;
        else if (lang === 'sh' || lang === 'bash' || lang === 'shell') execCmd = `bash ${pathOutput}`;
        else execCmd = `${pathOutput}`;

        log.info(`${ANSI.Cyan}[Execode]${ANSI.Reset} auto-executing generated file with: ${execCmd}`);
      }

      execResult = await execode(execCmd);
    } catch (err: any) {
      log.error(`${ANSI.Red}[Execode]${ANSI.Reset}`, err?.message ?? err);
      feedback = `Execution failed: ${err?.message ?? "unknown"}`;
      continue;
    }

    // Debug: log exec output and the written artifact to diagnose failures
    try {
      log.info(`${ANSI.Cyan}[Execode]${ANSI.Reset} exit=${execResult.exitCode}`);
      log.info(`${ANSI.Cyan}[Execode]${ANSI.Reset} stdout=${String(execResult.stdout).slice(0,1000)}`);
      log.info(`${ANSI.Cyan}[Execode]${ANSI.Reset} stderr=${String(execResult.stderr).slice(0,1000)}`);
      try {
        const outFile = await readFile(pathOutput, 'utf-8');
        log.info(`${ANSI.Cyan}[OutputFile]${ANSI.Reset} ${outFile.slice(0,1000)}`);
      } catch (err: any) {
        log.info(`${ANSI.Cyan}[OutputFile]${ANSI.Reset} could not read output file: ${err?.message ?? err}`);
      }
    } catch (errAny) {
      // swallow logging errors
    }

    // Validate execution results using the waterfall pipeline
    let verdict: any;
    try {
      if (signal?.aborted) throw new Error("Aborted");
      verdict = await waterfall({
        exec: execResult,
        artifacts: {
          outputFile: pathOutput,
        },
      });
    } catch (err: any) {
      log.error(`${ANSI.Red}[Waterfall]${ANSI.Reset}`, err?.message ?? err);
      feedback = `Validation pipeline failed: ${err?.message ?? "unknown"}`;
      continue;
    }

    // Stop if the execution is valid
    if (verdict?.ok) {
      log.info(`${ANSI.Green}[Agent]${ANSI.Reset} success after ${limit} iterations`);
      return { ok: true, iterations: limit, verdict, artifacts: { outputFile: pathOutput } };
    }

    // Build structured feedback for the next iteration
    feedback = `Layer: ${verdict?.layer ?? "unknown"}\nReason: ${verdict?.reason ?? "unknown"}\nDetails: ${verdict?.details ?? "n/a"}`;

    log.warn(`${ANSI.Yellow}[Waterfall]${ANSI.Reset} failed at ${verdict?.layer ?? "unknown"}`);
  }

  // If the loop completes without a successful verdict
  return { ok: false, iterations: limit };
};
